{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23cac34-f498-41e3-ae67-be0d37e11f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import allennlp\n",
    "from allennlp.data import Vocabulary, Instance\n",
    "from allennlp.data.fields import TextField, LabelField\n",
    "from simple_classifier.dataset_reader import YelpReviewJsonLinesReader\n",
    "from allennlp.data.data_loaders import SimpleDataLoader\n",
    "\n",
    "from simple_classifier.model import SimpleClassifier\n",
    "from allennlp.modules import Embedding\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.seq2vec_encoders import BagOfEmbeddingsEncoder\n",
    "from allennlp.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8010580-8fe3-44a1-ada8-ef7e1ecf5d73",
   "metadata": {},
   "source": [
    "# Intro\n",
    "This code is a simplified implementation of what happens during an AllenNLP training loop.\n",
    "Normally, you will NOT need to write code like this, since the \"train\" command will \n",
    "handle all of it for you based on what's in your config file. But we are going to show you\n",
    "the essence of what happens when you call the \"train\" command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8489d4b-b221-4249-b49d-aff000d9c00d",
   "metadata": {},
   "source": [
    "## Reading\n",
    "Here, we'll instantiate the `DatasetReader` we made for our Yelp review dataset. \n",
    "Note that we will not pass it any arguments, and instead use its default values for `tokenizer` and `token_indexers`.\n",
    "A `tokenizer` turns a string into a list of tokens. \n",
    "Don't worry about `token_indexers` for now--just know that it's used to turn tokens into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ca0b8-fa07-4324-9a81-6c8f7c5269ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = YelpReviewJsonLinesReader()\n",
    "print(\"Reader tokenizer:\", reader.tokenizer)\n",
    "print(\"Reader token indexers:\", reader.token_indexers)\n",
    "\n",
    "# Load our dev data and use list() to force it all into memory\n",
    "instances = list(reader.read('data/dev_500.jsonl'))\n",
    "sample_instance = instances[0]\n",
    "print(\"\\nExample instance:\\n\\n\", sample_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58278c2b-b281-4a84-9053-40791e1f0299",
   "metadata": {},
   "source": [
    "## Vocabulary Setup\n",
    "Our vocabulary helps us keep in a single place our mappings between strings and integers. \n",
    "Vocabularies can be thought of very simply as nested dictionaries with a depth of 2: the first key is the **namespace**, which helps keep logically distinct string values (e.g. POS tags and tokens) distinct; the second key is either the token, in which case the final value will be the token's corresponding integer value, or the integer value, in which case the value is the token's string value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c1ea2-0e3c-4bc7-b5df-8ac3fe01ad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary.from_instances(instances)\n",
    "print(\"Vocabulary summary:\\n\\n\", vocab)\n",
    "\n",
    "print(\"Label mapping:\\n\", vocab.get_index_to_token_vocabulary(\"labels\"))\n",
    "print(\"\\nFirst 10 token mappings:\\n\", {k: v for k, v in vocab.get_index_to_token_vocabulary(\"tokens\").items() if k < 10})\n",
    "\n",
    "print(\"\\nIndex for \\\"dog\\\" in namespace \\\"tokens\\\":\", vocab.get_token_index(\"dog\", namespace=\"tokens\"))\n",
    "print(\"\\nToken at index 540 in namespace \\\"tokens\\\":\", vocab.get_token_from_index(540))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60c67ec-ca8f-4fee-9669-13d35f83ba91",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "The next step in a training pipeline is to create a `DataLoader` which will sit on top of a list of `Instance`s and manage turning them into tensors.\n",
    "Note the complicated structure of `text`: don't worry about it for now, we'll handle it in our next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a1ef7-c13b-49cf-9bee-addb60af79a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = SimpleDataLoader(instances, batch_size=4, vocab=vocab)\n",
    "batches = [batch for batch in loader]\n",
    "sample_batch = batches[0]\n",
    "print(sample_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0244b-b365-4d6d-a920-7cb2ec41d6a7",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "Here we will load the model we trained in the previous section and use it to make predictions for our sample batch.\n",
    "Note the format of the `probs` key in the output: there is one row per item in the batch, and every column corresponds to a particular value in the `\"labels\"` namespace in `vocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4033d-ca78-4334-8092-dc3cb0e0e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "archive_path = 'model' + os.sep + 'model.tar.gz'\n",
    "model = Model.from_archive(archive_path)\n",
    "\n",
    "sample_output = model(text=sample_batch[\"text\"], label=sample_batch[\"label\"])\n",
    "print(sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc3b31-7bf9-4c4d-9882-9ffd0a7eadb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 13\n",
    "# ==============================\n",
    "new_instances = [\n",
    "    reader.text_to_instance(\"Bad and slow service, burger tastes like cardboard\", 0),\n",
    "    reader.text_to_instance(\"The food here is always so yummy and our kid always has a great time!\", 4)\n",
    "]\n",
    "\n",
    "def print_probs(instance):\n",
    "    output = model.forward_on_instance(instance)\n",
    "    class_probs = output[\"probs\"]\n",
    "    print(\"  Text:\", instance[\"text\"].tokens)\n",
    "    print(\"  Label:\", instance[\"label\"].label) \n",
    "    for rating in [\"1\", \"2\", \"3\", \"4\", \"5\"]:\n",
    "        index = vocab.get_token_index(rating, namespace=\"labels\")\n",
    "        print(f\"  {rating} stars: {class_probs[index].item() * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nFirst instance\")\n",
    "print_probs(new_instances[0])\n",
    "print(\"\\nSecond instance\")\n",
    "print_probs(new_instances[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36ff3c1-8882-4a9d-916a-d12feb0d1549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 14\n",
    "# ==============================\n",
    "vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
